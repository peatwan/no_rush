{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from operator import add\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(\"local\",'app')\n",
    "spark = SparkSession.builder.appName('name').config('spark.sql.shuffle.partitions',10).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load data\n",
    "## 1.1 big trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.read.csv('data/train_flight.csv',header=True,inferSchema=True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 small_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45616"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small=spark.read.csv('data/small.csv',header=True,inferSchema=True)\n",
    "data_small.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 choose one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4571729\n",
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      "\n",
      "+--------------+------------------+\n",
      "|SCHEDULED_TIME|schedule_departure|\n",
      "+--------------+------------------+\n",
      "|           110|               375|\n",
      "|           110|               375|\n",
      "|           111|               480|\n",
      "|           111|               480|\n",
      "|           111|               480|\n",
      "|           109|               636|\n",
      "|           109|               636|\n",
      "|           109|               636|\n",
      "|           109|               636|\n",
      "|           106|               728|\n",
      "|           106|               728|\n",
      "|           106|               728|\n",
      "|           106|               728|\n",
      "|           108|               860|\n",
      "|           108|               860|\n",
      "|           108|               860|\n",
      "|           108|               860|\n",
      "|           103|               985|\n",
      "|           103|               985|\n",
      "|           140|               485|\n",
      "+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # can be updated\n",
    "#dataset=data_small\n",
    "dataset=data\n",
    "print(dataset.count())\n",
    "dataset.printSchema()\n",
    "#dataset.select(\"SCHEDULED_TIME\",'schedule_departure').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570501"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just try airport clustering\n",
    "dataset=dataset.filter(dataset['AIRLINE']=='AA')\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 change label to classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "name = 'DEPARTURE_DELAY'\n",
    "\n",
    "def tran_label(element):\n",
    "    if element < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "udf = UserDefinedFunction(lambda x: tran_label(x), IntegerType())\n",
    "\n",
    "new_data=dataset.select('*',udf(dataset['DEPARTURE_DELAY']).alias('class_labels'))\n",
    "dataset=new_data.drop('DEPARTURE_DELAY')\n",
    "dataset=dataset.withColumnRenamed('class_labels','DEPARTURE_DELAY')\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 change label to doubletype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: double (nullable = true)\n",
      " |-- square_Schedule: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DoubleType,IntegerType\n",
    "name = 'DEPARTURE_DELAY'\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: x*1.0, DoubleType())\n",
    "new_data=dataset.select('*',udf(dataset['DEPARTURE_DELAY']).alias('double_labels'))\n",
    "dataset=new_data.drop('DEPARTURE_DELAY')\n",
    "dataset=dataset.withColumnRenamed('double_labels','DEPARTURE_DELAY')\n",
    "dataset.printSchema()\n",
    "# create a new feature\n",
    "udf = UserDefinedFunction(lambda x: x*x, IntegerType())\n",
    "new_data=dataset.select('*',udf(dataset['Schedule_departure']).alias('square_Schedule'))\n",
    "dataset=new_data\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 feature transformation pipeline\n",
    "## 2.1 feature selection (can be updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=data\n",
    "categoricalColumns = ['ORIGIN_AIRPORT']  # to add\n",
    "numericCols = ['schedule_departure','NEW_DAY','square_Schedule']  # to add\n",
    "# all_features=categoricalColumns+numericCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 transform and onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[141,186,187...|\n",
      "|(189,[162,186,187...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "cols=dataset.columns\n",
    "\n",
    "stages = [] \n",
    "feature_names=[]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, \n",
    "        outputCol=categoricalCol+\"Index\")\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", \n",
    "        outputCol=categoricalCol+\"classVec\")\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset_transformed = pipelineModel.transform(dataset)\n",
    "selectedcols = ['DEPARTURE_DELAY', \"features\"] \n",
    "dataset_transformed = dataset_transformed.select(selectedcols)\n",
    "dataset_transformed=dataset_transformed.select('*').withColumnRenamed('DEPARTURE_DELAY','label')\n",
    "dataset_transformed.printSchema()\n",
    "dataset_transformed.select('features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 sample and split dataset into trainingData and testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_used,left_behind=dataset_transformed.randomSplit((0.2,0.8),1)\n",
    "trainingData,testData=dataset_used.randomSplit((0.8,0.2),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['schedule_departure', 'NEW_DAY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(2, {0: 0.6792, 1: 0.3208})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf= RandomForestRegressor(numTrees=10, maxDepth=3, seed=42)\n",
    "model = rf.fit(dataset_transformed)\n",
    "print(assemblerInputs)\n",
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 machine learning model(by pyspark.ml.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 train model (RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "numFolds =2\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\")   \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees,[50]) \\\n",
    "    .build()\n",
    "crossval = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(),\n",
    "    numFolds=numFolds)\n",
    "model = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 linear family model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "# Isotonic\n",
    "model = IsotonicRegression(labelCol=\"label\", featuresCol=\"features\").fit(trainingData)\n",
    "\n",
    "#linear regression\n",
    "#lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,labelCol=\"label\", featuresCol=\"features\")\n",
    "#model = lr.fit(trainingData)\n",
    "\n",
    "# G LR, gaussian,\n",
    "#glr = GeneralizedLinearRegression(family=\"Tweedie\", maxIter=10, regParam=0.3,labelCol=\"label\", featuresCol=\"features\")\n",
    "#model = glr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================\n",
      "TrainingData count: 91378\n",
      "TestData count: 22915\n",
      "=====================================================================\n",
      "Training data MSE = 3793.87410536\n",
      "Training data RMSE = 61.5944324218\n",
      "Training data R-squared = -0.975014555477\n",
      "Training data MAE = 34.2013832651\n",
      "Training data Explained variance = 1997.64767123\n",
      "=====================================================================\n",
      "Validation data MSE = 3946.28448614\n",
      "Validation data RMSE = 62.8194594544\n",
      "Validation data R-squared = -1.08428008797\n",
      "Validation data MAE = 34.4607898756\n",
      "Validation data Explained variance = 2120.10973005\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "cvModel=model\n",
    "trainPredictionsAndLabels = cvModel.transform(trainingData).select(\"label\", \"prediction\").rdd\n",
    "validPredictionsAndLabels = cvModel.transform(testData).select(\"label\", \"prediction\").rdd\n",
    "trainRegressionMetrics = RegressionMetrics(trainPredictionsAndLabels)\n",
    "validRegressionMetrics = RegressionMetrics(validPredictionsAndLabels)\n",
    "\n",
    "bestModel = cvModel\n",
    "#featureImportances = bestModel.featureImportances.toArray()\n",
    "#print (featureImportances)\n",
    "\n",
    "output = str(\"\\n=====================================================================\\n\" +\n",
    "      \"TrainingData count: {0}\\n\".format(trainingData.count()) +\n",
    "      \"TestData count: {0}\\n\".format(testData.count()) +\n",
    "      \"=====================================================================\\n\" +\n",
    "      \"Training data MSE = {}\\n\".format(trainRegressionMetrics.meanSquaredError) +\n",
    "      \"Training data RMSE = {}\\n\".format(trainRegressionMetrics.rootMeanSquaredError) +\n",
    "      \"Training data R-squared = {}\\n\".format(trainRegressionMetrics.r2) +\n",
    "      \"Training data MAE = {}\\n\".format(trainRegressionMetrics.meanAbsoluteError) +\n",
    "      \"Training data Explained variance = {}\\n\".format(trainRegressionMetrics.explainedVariance) +\n",
    "      \"=====================================================================\\n\" +\n",
    "      \"Validation data MSE = {0}\\n\".format(validRegressionMetrics.meanSquaredError) +\n",
    "      \"Validation data RMSE = {0}\\n\".format(validRegressionMetrics.rootMeanSquaredError) +\n",
    "      \"Validation data R-squared = {0}\\n\".format(validRegressionMetrics.r2) +\n",
    "      \"Validation data MAE = {0}\\n\".format(validRegressionMetrics.meanAbsoluteError) +\n",
    "      \"Validation data Explained variance = {0}\\n\".format(validRegressionMetrics.explainedVariance) +\n",
    "     # \"=====================================================================\\n\" +\n",
    "      #\"CV params explained: {}\\n\".format(cvModel.explainParams()) +\n",
    "     # \"RandomForest params explained: {}\\n\".format(bestModel.explainParams()) +\n",
    "      #\"RandomForest features importances:\\n {0}\\n\".format(\"\\n\".join(map(lambda z: \"{0} = {1}\".format(str(z[0]),str(z[1])), zip(featureCols, featureImportances)))) +\n",
    "\"=====================================================================\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML model (by MLlib)\n",
    "## 3.1 generate RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change into RDD\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))\n",
    "        \n",
    "airlineRDD=dataset_transformed.rdd.map(lambda row: LabeledPoint(row['label'],as_mllib(row['features'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 split trainset and testset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Spliting dataset into train and test dtasets\n",
    "airlineRDD.cache()\n",
    "use_data,left_data=airlineRDD.randomSplit([0.2,0.8])\n",
    "trainingData,testData=use_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 use Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                        numTrees=100, featureSubsetStrategy=\"auto\",\n",
    "                                        impurity='variance', maxDepth=10, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "#predictions = model.predict(testData.map(lambda x: x.features))\n",
    "#labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "#testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "#       float(testData.count())\n",
    "#print('Test Mean Squared Error = ' + str(testMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1508.56180569\n",
      "RMSE = 38.8402086206\n",
      "R-squared = -33.1146795254\n",
      "MAE = 17.8823082125\n",
      "Explained variance = 1527.0073945\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "# Instantiate metrics object\n",
    "metrics = RegressionMetrics(labelsAndPredictions)\n",
    "# Squared Error\n",
    "print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    " # R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)\n",
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % metrics.explainedVariance)\n",
    "# exampleoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 use GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "model = GradientBoostedTrees.trainRegressor(trainingData,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.38938509204\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(trainingData,numClasses=3)\n",
    "print('finish training')\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andy/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "model = LinearRegressionWithSGD.train(trainRDD, iterations=100, step=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n",
    "valuesAndPreds = testRDD.map(lambda p: (p.label, model.predict(p.features)))\n",
    "MSE = valuesAndPreds \\\n",
    "    .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.5 save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "model.save(sc, \"model/pythonLinearRegressionWithSGDModel\")\n",
    "\n",
    "sameModel = LinearRegressionModel.load(sc, \"model/pythonLinearRegressionWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
