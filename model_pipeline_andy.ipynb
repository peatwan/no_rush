{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from operator import add\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from __future__ import print_function\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(appName=\"local\")\n",
    "spark = SparkSession.builder.appName('name').config('spark.sql.shuffle.partitions',5).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load data\n",
    "## 1.1 big trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.read.csv('data/train_flight.csv',header=True,inferSchema=True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 small_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45616"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small=spark.read.csv('data/small.csv',header=True,inferSchema=True)\n",
    "data_small.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 choose one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4571729\n",
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # can be updated\n",
    "#dataset=data_small\n",
    "dataset=data\n",
    "print(dataset.count())\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# just try airport clustering\n",
    "dataset=dataset.filter(dataset['AIRLINE']=='AA')\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 change label to classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "name = 'DEPARTURE_DELAY'\n",
    "\n",
    "def tran_label(element):\n",
    "    if element < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "udf = UserDefinedFunction(lambda x: tran_label(x), IntegerType())\n",
    "\n",
    "new_data=dataset.select('*',udf(dataset['DEPARTURE_DELAY']).alias('class_labels'))\n",
    "dataset=new_data.drop('DEPARTURE_DELAY')\n",
    "dataset=dataset.withColumnRenamed('class_labels','DEPARTURE_DELAY')\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 change label to doubletype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- schedule_departure: integer (nullable = true)\n",
      " |-- NEW_DAY: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DoubleType\n",
    "name = 'DEPARTURE_DELAY'\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: x*1.0, DoubleType())\n",
    "\n",
    "new_data=dataset.select('*',udf(dataset['DEPARTURE_DELAY']).alias('double_labels'))\n",
    "dataset=new_data.drop('DEPARTURE_DELAY')\n",
    "dataset=dataset.withColumnRenamed('double_labels','DEPARTURE_DELAY')\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 feature transformation pipeline\n",
    "## 2.1 feature selection (can be updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset=data\n",
    "categoricalColumns = ['ORIGIN_AIRPORT']  # to add\n",
    "numericCols = ['SCHEDULED_TIME','NEW_DAY','SCHEDULED_ARRIVAL','DISTANCE']  # to add\n",
    "# all_features=categoricalColumns+numericCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 transform and onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[243,627,628...|\n",
      "|(631,[333,627,628...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "cols=dataset.columns\n",
    "\n",
    "stages = [] \n",
    "feature_names=[]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, \n",
    "        outputCol=categoricalCol+\"Index\")\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", \n",
    "        outputCol=categoricalCol+\"classVec\")\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset_transformed = pipelineModel.transform(dataset)\n",
    "selectedcols = ['DEPARTURE_DELAY', \"features\"] \n",
    "dataset_transformed = dataset_transformed.select(selectedcols)\n",
    "dataset_transformed=dataset_transformed.select('*').withColumnRenamed('DEPARTURE_DELAY','label')\n",
    "dataset_transformed.printSchema()\n",
    "dataset_transformed.select('features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORIGIN_AIRPORTclassVec', 'SCHEDULED_TIME', 'NEW_DAY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(548,[1,27,36,57,143,295,321,322,400,526,546,547],[0.0408834454234,0.00876645216567,0.0327657812684,0.0767878620169,0.0108964305701,0.155255353251,0.0174407157481,0.0242766147333,0.0499435380631,0.0662880595107,0.063992319078,0.452703428171])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf= RandomForestRegressor(numTrees=5, maxDepth=3, seed=42)\n",
    "model = rf.fit(dataset_transformed)\n",
    "print assemblerInputs\n",
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 use ML library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 train model (RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o224.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 19.0 failed 1 times, most recent failure: Lost task 2.0 in stage 19.0 (TID 56, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\nPrevious exception in task: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\torg.apache.spark.storage.StorageUtils$.cleanDirectBuffer(StorageUtils.scala:293)\n\torg.apache.spark.storage.StorageUtils$.dispose(StorageUtils.scala:288)\n\torg.apache.spark.storage.StorageUtils.dispose(StorageUtils.scala)\n\torg.apache.spark.io.NioBufferedFileInputStream.close(NioBufferedFileInputStream.java:130)\n\tjava.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n\torg.spark_project.guava.io.Closeables.close(Closeables.java:77)\n\torg.apache.spark.sql.execution.python.DiskRowQueue.close(RowQueue.scala:152)\n\torg.apache.spark.sql.execution.python.HybridRowQueue.remove(RowQueue.scala:258)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:166)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:158)\n\tscala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter1$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\torg.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1799)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\torg.apache.spark.scheduler.Task.run(Task.scala:108)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\tjava.base/java.lang.Thread.run(Thread.java:844)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:118)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:118)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:130)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\nPrevious exception in task: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\torg.apache.spark.storage.StorageUtils$.cleanDirectBuffer(StorageUtils.scala:293)\n\torg.apache.spark.storage.StorageUtils$.dispose(StorageUtils.scala:288)\n\torg.apache.spark.storage.StorageUtils.dispose(StorageUtils.scala)\n\torg.apache.spark.io.NioBufferedFileInputStream.close(NioBufferedFileInputStream.java:130)\n\tjava.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n\torg.spark_project.guava.io.Closeables.close(Closeables.java:77)\n\torg.apache.spark.sql.execution.python.DiskRowQueue.close(RowQueue.scala:152)\n\torg.apache.spark.sql.execution.python.HybridRowQueue.remove(RowQueue.scala:258)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:166)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:158)\n\tscala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter1$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\torg.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1799)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\torg.apache.spark.scheduler.Task.run(Task.scala:108)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\tjava.base/java.lang.Thread.run(Thread.java:844)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:118)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-27e430e10166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     numFolds=numFolds)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/ml/tuning.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparamMap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o224.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 19.0 failed 1 times, most recent failure: Lost task 2.0 in stage 19.0 (TID 56, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\nPrevious exception in task: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\torg.apache.spark.storage.StorageUtils$.cleanDirectBuffer(StorageUtils.scala:293)\n\torg.apache.spark.storage.StorageUtils$.dispose(StorageUtils.scala:288)\n\torg.apache.spark.storage.StorageUtils.dispose(StorageUtils.scala)\n\torg.apache.spark.io.NioBufferedFileInputStream.close(NioBufferedFileInputStream.java:130)\n\tjava.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n\torg.spark_project.guava.io.Closeables.close(Closeables.java:77)\n\torg.apache.spark.sql.execution.python.DiskRowQueue.close(RowQueue.scala:152)\n\torg.apache.spark.sql.execution.python.HybridRowQueue.remove(RowQueue.scala:258)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:166)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:158)\n\tscala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter1$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\torg.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1799)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\torg.apache.spark.scheduler.Task.run(Task.scala:108)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\tjava.base/java.lang.Thread.run(Thread.java:844)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:118)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:118)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:130)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\nPrevious exception in task: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n\torg.apache.spark.storage.StorageUtils$.cleanDirectBuffer(StorageUtils.scala:293)\n\torg.apache.spark.storage.StorageUtils$.dispose(StorageUtils.scala:288)\n\torg.apache.spark.storage.StorageUtils.dispose(StorageUtils.scala)\n\torg.apache.spark.io.NioBufferedFileInputStream.close(NioBufferedFileInputStream.java:130)\n\tjava.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n\torg.spark_project.guava.io.Closeables.close(Closeables.java:77)\n\torg.apache.spark.sql.execution.python.DiskRowQueue.close(RowQueue.scala:152)\n\torg.apache.spark.sql.execution.python.HybridRowQueue.remove(RowQueue.scala:258)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:166)\n\torg.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7.apply(BatchEvalPythonExec.scala:158)\n\tscala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter1$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\torg.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1799)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\torg.apache.spark.scheduler.Task.run(Task.scala:108)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\tjava.base/java.lang.Thread.run(Thread.java:844)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:118)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "dataset_used,left_behind=dataset_transformed.randomSplit((0.01,0.99),1)\n",
    "\n",
    "numFolds =2\n",
    "trainingData,testData=dataset_used.randomSplit((0.8,0.2),1)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\")   \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees,[100]) \\\n",
    "    .build()\n",
    "crossval = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(),\n",
    "    numFolds=numFolds)\n",
    "\n",
    "model = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================\n",
      "TrainingData count: 36502\n",
      "TestData count: 9114\n",
      "=====================================================================\n",
      "Training data MSE = 1432.04823345\n",
      "Training data RMSE = 37.8424131557\n",
      "Training data R-squared = -84.3787281914\n",
      "Training data MAE = 18.7357809654\n",
      "Training data Explained variance = 1496.97441388\n",
      "=====================================================================\n",
      "Validation data MSE = 1224.12882869\n",
      "Validation data RMSE = 34.9875524822\n",
      "Validation data R-squared = -188.941401709\n",
      "Validation data MAE = 18.4183763529\n",
      "Validation data Explained variance = 1229.21362335\n",
      "=====================================================================\n",
      "CV params explained: estimator: estimator to be cross-validated (current: RandomForestRegressor_4ae499c2617bddfa4284)\n",
      "estimatorParamMaps: estimator param maps (current: [{Param(parent=u'RandomForestRegressor_4ae499c2617bddfa4284', name='numTrees', doc='Number of trees to train (>= 1).'): 100}])\n",
      "evaluator: evaluator used to select hyper-parameters that maximize the validator metric (current: RegressionEvaluator_4e9699f7e2bc594d0c94)\n",
      "seed: random seed. (default: -4372709618522015412)\n",
      "RandomForest params explained: \n",
      "=====================================================================\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/kernel-PySpark-d944dc6b-9399-4682-9f72-3a2f5e8ab9c1/pyspark_runner.py\", line 196, in <module>\n",
      "    output.reset()\n",
      "AttributeError: 'str' object has no attribute 'reset'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "cvModel=model\n",
    "trainPredictionsAndLabels = cvModel.transform(trainingData).select(\"label\", \"prediction\").rdd\n",
    "validPredictionsAndLabels = cvModel.transform(testData).select(\"label\", \"prediction\").rdd\n",
    "trainRegressionMetrics = RegressionMetrics(trainPredictionsAndLabels)\n",
    "validRegressionMetrics = RegressionMetrics(validPredictionsAndLabels)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "featureImportances = bestModel.featureImportances.toArray()\n",
    "print featureImportances\n",
    "\n",
    "output = str(\"\\n=====================================================================\\n\" +\n",
    "      \"TrainingData count: {0}\\n\".format(trainingData.count()) +\n",
    "      \"TestData count: {0}\\n\".format(testData.count()) +\n",
    "      \"=====================================================================\\n\" +\n",
    "      \"Training data MSE = {}\\n\".format(trainRegressionMetrics.meanSquaredError) +\n",
    "      \"Training data RMSE = {}\\n\".format(trainRegressionMetrics.rootMeanSquaredError) +\n",
    "      \"Training data R-squared = {}\\n\".format(trainRegressionMetrics.r2) +\n",
    "      \"Training data MAE = {}\\n\".format(trainRegressionMetrics.meanAbsoluteError) +\n",
    "      \"Training data Explained variance = {}\\n\".format(trainRegressionMetrics.explainedVariance) +\n",
    "      \"=====================================================================\\n\" +\n",
    "      \"Validation data MSE = {0}\\n\".format(validRegressionMetrics.meanSquaredError) +\n",
    "      \"Validation data RMSE = {0}\\n\".format(validRegressionMetrics.rootMeanSquaredError) +\n",
    "      \"Validation data R-squared = {0}\\n\".format(validRegressionMetrics.r2) +\n",
    "      \"Validation data MAE = {0}\\n\".format(validRegressionMetrics.meanAbsoluteError) +\n",
    "      \"Validation data Explained variance = {0}\\n\".format(validRegressionMetrics.explainedVariance) +\n",
    "      \"=====================================================================\\n\" +\n",
    "      \"CV params explained: {}\\n\".format(cvModel.explainParams()) +\n",
    "      \"RandomForest params explained: {}\\n\".format(bestModel.explainParams()) +\n",
    "      #\"RandomForest features importances:\\n {0}\\n\".format(\"\\n\".join(map(lambda z: \"{0} = {1}\".format(str(z[0]),str(z[1])), zip(featureCols, featureImportances)))) +\n",
    "\"=====================================================================\\n\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML model (by MLlib)\n",
    "## 3.1 generate RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change into RDD\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))\n",
    "        \n",
    "airlineRDD=dataset_transformed.rdd.map(lambda row: LabeledPoint(row['label'],as_mllib(row['features'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 split trainset and testset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Spliting dataset into train and test dtasets\n",
    "airlineRDD.cache()\n",
    "use_data,left_data=airlineRDD.randomSplit([0.01,0.99])\n",
    "trainingData,testData=use_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 use Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                        numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                        impurity='variance', maxDepth=4, maxBins=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 1359.99922727\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "        float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1359.99922727\n",
      "RMSE = 36.8781673524\n",
      "R-squared = -88.29262504\n",
      "MAE = 18.4746674913\n",
      "Explained variance = 1366.23195794\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "#valuesAndPreds = testData.map(lambda p: (model.predict(p.features), p.label))\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "# Instantiate metrics object\n",
    "metrics = RegressionMetrics(labelsAndPredictions)\n",
    "    # Squared Error\n",
    "print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "    # R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)\n",
    "\n",
    "    # Mean absolute error\n",
    "print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "\n",
    "    # Explained variance\n",
    "print(\"Explained variance = %s\" % metrics.explainedVariance)\n",
    "    # exampleoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 use GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.619792551291\n",
      "Learned classification GBT model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 187 <= 235.0)\n",
      "     If (feature 0 <= 0.0)\n",
      "      If (feature 2 <= 0.0)\n",
      "       Predict: 0.20240178145379353\n",
      "      Else (feature 2 > 0.0)\n",
      "       Predict: 0.5941038239692373\n",
      "     Else (feature 0 > 0.0)\n",
      "      If (feature 187 <= 16.0)\n",
      "       Predict: 1.0702360391479562\n",
      "      Else (feature 187 > 16.0)\n",
      "       Predict: 0.5228811820233942\n",
      "    Else (feature 187 > 235.0)\n",
      "     If (feature 187 <= 355.0)\n",
      "      If (feature 2 <= 0.0)\n",
      "       Predict: -0.04078576685870917\n",
      "      Else (feature 2 > 0.0)\n",
      "       Predict: 0.412833776038026\n",
      "     Else (feature 187 > 355.0)\n",
      "      If (feature 3 <= 0.0)\n",
      "       Predict: 0.7047692439421184\n",
      "      Else (feature 3 > 0.0)\n",
      "       Predict: 1.110079575596817\n",
      "  Tree 1:\n",
      "    If (feature 187 <= 236.0)\n",
      "     If (feature 1 <= 0.0)\n",
      "      If (feature 0 <= 0.0)\n",
      "       Predict: -0.7436935228217824\n",
      "      Else (feature 0 > 0.0)\n",
      "       Predict: -1.1530598227282285\n",
      "     Else (feature 1 > 0.0)\n",
      "      If (feature 187 <= 15.0)\n",
      "       Predict: 0.7491868127242968\n",
      "      Else (feature 187 > 15.0)\n",
      "       Predict: -0.1016368328619418\n",
      "    Else (feature 187 > 236.0)\n",
      "     If (feature 187 <= 356.0)\n",
      "      If (feature 2 <= 0.0)\n",
      "       Predict: 0.0872942641003491\n",
      "      Else (feature 2 > 0.0)\n",
      "       Predict: -1.0536721301792282\n",
      "     Else (feature 187 > 356.0)\n",
      "      If (feature 186 <= 114.0)\n",
      "       Predict: -1.3884806665519946\n",
      "      Else (feature 186 > 114.0)\n",
      "       Predict: -1.1776750683151227\n",
      "  Tree 2:\n",
      "    If (feature 187 <= 194.0)\n",
      "     If (feature 2 <= 0.0)\n",
      "      If (feature 0 <= 0.0)\n",
      "       Predict: -0.3819548178228466\n",
      "      Else (feature 0 > 0.0)\n",
      "       Predict: -0.9110495119010494\n",
      "     Else (feature 2 > 0.0)\n",
      "      If (feature 187 <= 17.0)\n",
      "       Predict: -0.8296800961515131\n",
      "      Else (feature 187 > 17.0)\n",
      "       Predict: -1.2491027331738551\n",
      "    Else (feature 187 > 194.0)\n",
      "     If (feature 187 <= 355.0)\n",
      "      If (feature 187 <= 247.0)\n",
      "       Predict: -0.2790555878696901\n",
      "      Else (feature 187 > 247.0)\n",
      "       Predict: 0.05445489269576601\n",
      "     Else (feature 187 > 355.0)\n",
      "      If (feature 7 <= 0.0)\n",
      "       Predict: -1.0880388984684424\n",
      "      Else (feature 7 > 0.0)\n",
      "       Predict: -1.70187535781241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.38938509204\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(trainingData,numClasses=3)\n",
    "print('finish training')\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling o4436.trainSVMModelWithSGD.\n",
       ": org.apache.spark.SparkException: Input validation failed.\n",
       "\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:256)\n",
       "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n",
       "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:248)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o4436.trainSVMModelWithSGD.\\n', JavaObject id=o4439), <traceback object at 0x7fd032722488>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:157)\n",
       "sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(trainingData, iterations=100)\n",
    "print('finish training')\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andy/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "model = LinearRegressionWithSGD.train(trainRDD, iterations=100, step=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n",
    "valuesAndPreds = testRDD.map(lambda p: (p.label, model.predict(p.features)))\n",
    "MSE = valuesAndPreds \\\n",
    "    .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.5 save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "model.save(sc, \"model/pythonLinearRegressionWithSGDModel\")\n",
    "\n",
    "sameModel = LinearRegressionModel.load(sc, \"model/pythonLinearRegressionWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
